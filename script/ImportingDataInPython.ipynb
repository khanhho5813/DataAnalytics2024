{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from 2017-18 Civil Rights Data Collection (CRDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Advanced Mathematics Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Advanced Placement Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Algebra I Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Algebra II Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Biology Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Calculus Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Chemistry Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Advanced Mathematics Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Advanced Placement Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Algebra I Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Algebra II Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Biology Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Calculus Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Chemistry Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Corporal Punishment Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Credit Recovery Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Dual Enrollment Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Enrollment Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Expulsions Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Geometry Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Gifted and Talented Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Harassment and Bullying Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_International Baccalaureate Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Justice Facilities Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Offenses Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Physics Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Referrals and Arrests Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Restraint and Seclusion Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Retention Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_SAT and ACT Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_School Characteristics Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_School Expenditures Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_School Support Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Single-sex Athletics Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Single-sex Classes Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Suspensions Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\cleaned_Transfers Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Corporal Punishment Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Credit Recovery Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Dual Enrollment Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Enrollment Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Expulsions Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Geometry Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Gifted and Talented Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Harassment and Bullying Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\International Baccalaureate Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Justice Facilities Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Offenses Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Physics Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Referrals and Arrests Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Restraint and Seclusion Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Retention Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\SAT and ACT Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\School Characteristics Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\School Expenditures Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\School Support Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Single-sex Athletics Cleaned.csv\n",
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Single-sex Classes Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Suspensions Cleaned.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bioma\\AppData\\Local\\Temp\\ipykernel_7568\\2299907655.py:14: DtypeWarning: Columns (2,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(original_csv_path, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV saved to: ../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned\\Transfers Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_csv(input_directory, output_directory, encoding='ISO-8859-1'):\n",
    "    # Iterate through all files in the input directory\n",
    "    for file_name in os.listdir(input_directory):\n",
    "        # Check if the file is a CSV\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Construct the full path to the file\n",
    "            original_csv_path = os.path.join(input_directory, file_name)\n",
    "            cleaned_csv_path = os.path.join(output_directory, file_name.replace('.csv', ' Cleaned.csv'))\n",
    "\n",
    "            # Load the CSV file with specified encoding\n",
    "            df = pd.read_csv(original_csv_path, encoding=encoding)\n",
    "\n",
    "            # Drop any rows that have all null values\n",
    "            df_cleaned = df.dropna(how='all')\n",
    "\n",
    "            # Save the cleaned data to a new CSV file\n",
    "            df_cleaned.to_csv(cleaned_csv_path, index=False)\n",
    "\n",
    "            print(f\"Cleaned CSV saved to: {cleaned_csv_path}\")\n",
    "\n",
    "# Define the input and output directories\n",
    "input_dir = '../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV'\n",
    "output_dir = '../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "# Call the function to clean all CSV files in the specified directory\n",
    "clean_csv(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"127.0.0.1\", port 5432 failed: fe_sendauth: no password supplied\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Establish the database connection\u001b[39;00m\n\u001b[0;32m     15\u001b[0m conn_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2017-18_Civil_Rights_Data_Collection\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgres\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mport\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5432\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# default PostgreSQL port is 5432\u001b[39;00m\n\u001b[0;32m     21\u001b[0m }\n\u001b[1;32m---> 23\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpsycopg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconn_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Function to guess data types of dataframe columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dropbox\\!JBGCourses\\!2024\\2024 Data Analytics\\Lectures\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     kwasync[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masync_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    121\u001b[0m dsn \u001b[38;5;241m=\u001b[39m _ext\u001b[38;5;241m.\u001b[39mmake_dsn(dsn, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 122\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcursor_factory \u001b[38;5;241m=\u001b[39m cursor_factory\n",
      "\u001b[1;31mOperationalError\u001b[0m: connection to server at \"127.0.0.1\", port 5432 failed: fe_sendauth: no password supplied\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import re  # Import the re module\n",
    "from glob import glob  # Corrected import statement\n",
    "from psycopg2 import sql\n",
    "from psycopg2.errors import StringDataRightTruncation\n",
    "\n",
    "# Retrieve the PostgreSQL_PWD environment variable\n",
    "postgresql_pwd = os.getenv('PostgreSQL_PWD')\n",
    "\n",
    "# Directory containing cleaned CSV files\n",
    "input_dir = '../Data/2017-18-crdc-data/2017-18 Public-Use Files/Data/SCH/CRDC/CSV Cleaned'\n",
    "\n",
    "# Establish the database connection\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',\n",
    "    \"password\": postgresql_pwd,\n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Function to guess data types of dataframe columns\n",
    "def guess_data_types(df):\n",
    "    column_types = {}\n",
    "    for column in df.columns:\n",
    "        unique_values = pd.Series(df[column].dropna().unique())\n",
    "        max_length = df[column].astype(str).map(len).max()\n",
    "        \n",
    "\n",
    "        # Check for leading zeros or alphanumeric content\n",
    "        has_leading_zeros = any(str(value).startswith('0') for value in unique_values.astype(str))\n",
    "        is_alphanumeric = any(not value.isdigit() for value in unique_values.astype(str))\n",
    "        # Check if the column name ends with 'id', 'key' or 'name (case-insensitive)\n",
    "        ends_with_id = column.lower().endswith('id')\n",
    "        ends_with_key = column.lower().endswith('key')\n",
    "        ends_with_name = column.lower().endswith('name')\n",
    "        starts_with_sch_ = column.lower().startswith('sch_')\n",
    "        is_numeric = pd.to_numeric(df[column], errors='coerce').notna().all()\n",
    "\n",
    "        \n",
    "        if ends_with_name:\n",
    "            column_types[column] = f'VARCHAR({len(\"Fenton STEM Academy: Elementary Center for Science Technology Engineering and Mathematics\")})'\n",
    "            continue  # Skip to the next column\n",
    "        \n",
    "        if starts_with_sch_ and is_numeric:\n",
    "            column_types[column] = 'DECIMAL'\n",
    "            continue\n",
    "\n",
    "        if has_leading_zeros or is_alphanumeric or ends_with_id or ends_with_key:\n",
    "            column_types[column] = f'VARCHAR({max_length+2})'\n",
    "            continue  # Skip to the next column\n",
    "        \n",
    "        \n",
    "        # Check for boolean and numeric types\n",
    "        if unique_values.isin([0, 1]).all():\n",
    "            column_types[column] = 'BOOLEAN'\n",
    "        elif all(pd.to_numeric(unique_values, errors='coerce').notna()):\n",
    "            if (df[column].astype(float) % 1 == 0).all() and max_length <= 10:\n",
    "                column_types[column] = 'INTEGER'\n",
    "            else:\n",
    "                column_types[column] = 'DECIMAL'\n",
    "        else:\n",
    "            column_types[column] = f'VARCHAR({max_length + 10})'\n",
    "\n",
    "    return column_types\n",
    "\n",
    "# Function to clean and standardize column names\n",
    "def clean_column_name(column_name):\n",
    "    cleaned_name = re.sub(r'\\W+', '_', column_name).strip('_').lower()\n",
    "    return cleaned_name if cleaned_name else 'invalid_column_name'\n",
    "\n",
    "# Function to adjust column length\n",
    "def adjust_column_length(table_name, column_name, increment=100):\n",
    "    # Query to get the current maximum length of the VARCHAR column\n",
    "    cursor.execute(sql.SQL(\"SELECT character_maximum_length FROM information_schema.columns WHERE table_name = {} AND column_name = {};\").format(\n",
    "        sql.Literal(table_name),\n",
    "        sql.Literal(column_name)\n",
    "    ))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        current_length = result[0]\n",
    "        new_length = current_length + increment\n",
    "        # Alter table SQL to adjust the column length\n",
    "        cursor.execute(sql.SQL(\"ALTER TABLE {} ALTER COLUMN {} TYPE VARCHAR({});\").format(\n",
    "            sql.Identifier(table_name),\n",
    "            sql.Identifier(column_name),\n",
    "            sql.Literal(new_length)\n",
    "        ))\n",
    "        conn.commit()\n",
    "\n",
    "# Process CSV files\n",
    "for file_path in glob(os.path.join(input_dir, '*.csv')):\n",
    "    try:\n",
    "        df_sample = pd.read_csv(file_path, nrows=9000)\n",
    "        column_types = guess_data_types(df_sample)\n",
    "        \n",
    "        table_name = os.path.basename(file_path).replace('.csv', '').replace(' ', '_').replace('-', '_').lower()\n",
    "        #print('\\n', table_name, column_types)\n",
    "        # Drop the existing table if it exists\n",
    "        cursor.execute(sql.SQL(\"DROP TABLE IF EXISTS {};\").format(sql.Identifier(table_name)))\n",
    "        conn.commit()\n",
    "\n",
    "        # Create the table\n",
    "        columns_sql = ', '.join([f'\"{clean_column_name(col)}\" {col_type}' for col, col_type in column_types.items() if clean_column_name(col) != 'invalid_column_name'])\n",
    "        cursor.execute(sql.SQL(\"CREATE TABLE {} ({});\").format(sql.Identifier(table_name), sql.SQL(columns_sql)))\n",
    "        conn.commit()\n",
    "\n",
    "        # Copy data from CSV file into the newly created table\n",
    "        with open(file_path, 'r') as f:\n",
    "            cursor.copy_expert(sql=sql.SQL(\"COPY {} FROM STDIN WITH CSV HEADER DELIMITER ',';\").format(sql.Identifier(table_name)), file=f)\n",
    "            conn.commit()\n",
    "    except psycopg2.errors.StringDataRightTruncation as e:\n",
    "        conn.rollback()\n",
    "        # Simplified extraction of column name from error message, assuming the format \"column \\\"column_name\\\"\"\n",
    "        match = re.search(r'column \\\"(.*?)\\\"', str(e))\n",
    "        if match:\n",
    "            column_name = match.group(1)\n",
    "            # Adjust the length of the problematic column\n",
    "            adjust_column_length(table_name, column_name)\n",
    "            # Retry the copy operation\n",
    "            with open(file_path, 'r') as f:\n",
    "                cursor.copy_expert(sql=sql.SQL(\"COPY {} FROM STDIN WITH CSV HEADER DELIMITER ',';\").format(sql.Identifier(table_name)), file=f)\n",
    "                conn.commit()\n",
    "        else:\n",
    "            print(f\"Could not parse column name from error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed processing {file_path}: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Connection parameters \n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  # or your username\n",
    "    \"password\": 'your_password',  # replace with your password\n",
    "    \"host\": '127.0.0.1'\n",
    "}\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',\n",
    "    \"password\": 'your_password',\n",
    "    \"host\": '127.0.0.1'\n",
    "}\n",
    "\n",
    "def fetch_and_compare_headers():\n",
    "    with psycopg2.connect(**conn_params) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"\"\"\n",
    "                SELECT table_name\n",
    "                FROM information_schema.tables\n",
    "                WHERE table_schema = 'public'\n",
    "                AND table_type = 'BASE TABLE';\n",
    "            \"\"\")\n",
    "            tables = cur.fetchall()\n",
    "\n",
    "            for (table_name,) in tables:\n",
    "                # Retrieve column headers for each table\n",
    "                cur.execute(sql.SQL(\"SELECT * FROM {} LIMIT 0;\").format(sql.Identifier(table_name)))\n",
    "                headers = [desc[0] for desc in cur.description]\n",
    "\n",
    "                # Retrieve the total number of rows for each table\n",
    "                cur.execute(sql.SQL(\"SELECT COUNT(*) FROM {};\").format(sql.Identifier(table_name)))\n",
    "                row_count = cur.fetchone()[0]\n",
    "\n",
    "                print(f\"\\nTable: {table_name}\")\n",
    "                print(\"Headers: \" + \", \".join(headers))\n",
    "                print(f\"Row Count: {row_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_and_compare_headers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data from 2017 School Locations & Geoassignments (SLGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/Users/abigailiovino/Desktop/Datathon/data/EDGE_GEOCODE_PUBLICLEA_1718/EDGE_GEOCODE_PUBLICLEA_1718.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Remove any blank rows or columns\n",
    "df = df.dropna()\n",
    "\n",
    "# Save the file as CSV\n",
    "csv_file_path = '/Users/abigailiovino/Desktop/Datathon/data/EDGE_GEOCODE_PUBLICLEA_1718/EDGE_GEOCODE_PUBLICLEA_1718.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from the Small Area Income and Poverty Estimates (SAIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We need to make sure we retain the leading zeroes in the district id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace this with the path to your actual Excel file\n",
    "excel_file_path = '/Users/abigailiovino/Desktop/Datathon/data/ussd17.xls'\n",
    "\n",
    "# The header is at the third row \n",
    "header_row_index = 2\n",
    "\n",
    "try:\n",
    "    # First, read the file with default settings to capture the header\n",
    "    temp_df = pd.read_excel(excel_file_path, header=header_row_index, nrows=0)  # nrows=0 loads no data, only headers\n",
    "    \n",
    "    # Identify the column names for \"State FIPS Code\" and \"District ID\"\n",
    "    # Assuming the column names that contain these IDs include \"State FIPS Code\" and \"District ID\"\n",
    "    state_fips_col_name = [col for col in temp_df.columns if \"State FIPS Code\" in col][0]\n",
    "    district_id_col_name = [col for col in temp_df.columns if \"District ID\" in col][0]\n",
    "\n",
    "    # Use converters to specify that these columns should be read as strings\n",
    "    converters_spec = {\n",
    "        state_fips_col_name: lambda x: str(x),\n",
    "        district_id_col_name: lambda x: str(x)\n",
    "    }\n",
    "    \n",
    "    # Now read the Excel file into a pandas DataFrame with the converters applied\n",
    "    poverty_data_df = pd.read_excel(excel_file_path, header=header_row_index, converters=converters_spec)\n",
    "    \n",
    "    # Replace any placeholder values (e.g., -9) that indicate missing or inapplicable data with NaN\n",
    "    poverty_data_df.replace(-9, pd.NA, inplace=True)\n",
    "    \n",
    "    # Print out the first few rows of the DataFrame to confirm it's been read correctly\n",
    "    print(poverty_data_df.head())\n",
    "except Exception as e:\n",
    "    # If there's an error, print out the error message\n",
    "    print(f\"An error occurred while reading the Excel file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(column_name):\n",
    "    cleaned_name = re.sub(r'\\W+', '_', column_name).strip('_').lower()\n",
    "    return cleaned_name if cleaned_name else 'invalid_column_name'\n",
    "\n",
    "# Function to guess data types for SQL, with special handling for 'id' columns\n",
    "def guess_data_types(df):\n",
    "    column_types = {}\n",
    "    for col in df.columns:\n",
    "        cleaned_col = clean_column_name(col)\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        max_length = df[col].astype(str).map(len).max()\n",
    "\n",
    "        # Handling for 'id' columns or columns with alphanumeric values\n",
    "        if cleaned_col.endswith('id') or cleaned_col.endswith('code') or cleaned_col.endswith('name'):\n",
    "            column_types[cleaned_col] = f'VARCHAR({max_length})'\n",
    "        else:\n",
    "            column_types[cleaned_col] = 'INTEGER'\n",
    "    return column_types\n",
    "\n",
    "# Function to read, clean, and return a DataFrame from an Excel file\n",
    "def read_and_clean_excel(file_path, header_row_index=2):\n",
    "    df = pd.read_excel(file_path, header=header_row_index)\n",
    "    df_cleaned = df.dropna(how='all')\n",
    "    return df_cleaned\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "# Establish database connection\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Use the functions defined earlier to create the SQL CREATE TABLE command\n",
    "column_types = guess_data_types(poverty_data_df)\n",
    "print(column_types)\n",
    "columns_sql = ', '.join([f'\"{clean_column_name(col)}\" {col_type}' for col, col_type in column_types.items()])\n",
    "\n",
    "table_name = 'school_poverty_data'  # Name your table\n",
    "try:\n",
    "    # Drop the existing table if it exists\n",
    "    cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\";')\n",
    "    conn.commit()\n",
    "\n",
    "    # Create the table\n",
    "    create_table_sql = f'CREATE TABLE \"{table_name}\" ({columns_sql});'\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    for index, row in poverty_data_df.iterrows():\n",
    "        columns = ', '.join([f'\"{clean_column_name(col)}\"' for col in row.index])\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        insert_sql = f'INSERT INTO \"{table_name}\" ({columns}) VALUES ({values});'\n",
    "        cursor.execute(insert_sql, tuple(row.values))\n",
    "\n",
    "    conn.commit()  # Commit changes\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    conn.rollback()  # Rollback in case of error\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# Activate the pandas2ri interface\n",
    "pandas2ri.activate()\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "\n",
    "# Query the data from your PostgreSQL table into a pandas DataFrame\n",
    "query = \"SELECT * FROM school_poverty_data;\"  # Replace with your actual query\n",
    "poverty_data_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Ensure the connnection is closed\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the ratio\n",
    "poverty_data_df['poverty_ratio'] = (\n",
    "    poverty_data_df['estimated_number_of_relevant_children_5_to_17_years_old_in_pove'] / \n",
    "    poverty_data_df['estimated_population_5_17']\n",
    ")\n",
    "\n",
    "# Print the ratios for each district\n",
    "poverty_data_df[['name', 'poverty_ratio']]\n",
    "\n",
    "# Plot the distribution of poverty ratios\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(poverty_data_df['poverty_ratio'], bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Poverty Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Poverty Ratios')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load R\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i poverty_data_df\n",
    "\n",
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "\n",
    "# Load the ggplot2 library\n",
    "library(ggplot2)\n",
    "\n",
    "# Create the histogram\n",
    "ggplot(poverty_data_df, aes(x = poverty_ratio)) +\n",
    "  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\", alpha = 0.8) +\n",
    "  labs(x = \"Poverty Ratio\", y = \"Frequency\", title = \"Distribution of Poverty Ratios\") +\n",
    "  theme_minimal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0`.1'\n",
    "}\n",
    "conn_dsn = \" \".join([f\"{key}='{value}'\" for key, value in conn_params.items()])\n",
    "\n",
    "# Establish database connection\n",
    "conn = psycopg2.connect(conn_dsn)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Step 1: Identify LEAIDs without a corresponding concat_id\n",
    "query_missing_leaids = \"\"\"\n",
    "WITH unique_leaids AS (\n",
    "    SELECT LEAID FROM advanced_mathematics_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM advanced_placement_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM algebra_i_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM algebra_ii\n",
    "    UNION\n",
    "    SELECT LEAID FROM algebra_ii_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM biology_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM calculus_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM chemistry_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM corporal_punishment_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM credit_recovery_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM dual_enrollment_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM enrollment_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM expulsions_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM geometry_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM gifted_and_talented_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM harassment_and_bullying_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM international_baccalaureate_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM justice_facilities_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM offenses_cleaned\n",
    "    UNION\n",
    "    SELECT LEAID FROM physics_cleaned\n",
    "),\n",
    "school_poverty_concat AS (\n",
    "    SELECT\n",
    "        sp.district_id,\n",
    "        sp.state_fips_code,\n",
    "        COALESCE(TRIM(CAST(sp.state_fips_code AS TEXT)), '') || COALESCE(TRIM(CAST(sp.district_id AS TEXT)), '') AS concat_id\n",
    "    FROM school_poverty_data sp\n",
    ")\n",
    "SELECT ul.LEAID\n",
    "FROM unique_leaids ul\n",
    "LEFT JOIN school_poverty_concat spc ON ul.LEAID = spc.concat_id\n",
    "WHERE spc.concat_id IS NULL;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query_missing_leaids)\n",
    "missing_leaids = sorted(cursor.fetchall())  # Sorting the missing LEAIDs\n",
    "print(len(missing_leaids))\n",
    "\n",
    "# Dictionary to store each LEAID associated with a set of unique school names\n",
    "leaids_school_names = {}\n",
    "\n",
    "# Define the tables to search\n",
    "tables_to_search = [\n",
    "    'advanced_mathematics_cleaned', 'advanced_placement_cleaned', 'algebra_i_cleaned', 'algebra_ii_cleaned',\n",
    "    'biology_cleaned', 'calculus_cleaned', 'chemistry_cleaned', 'corporal_punishment_cleaned',\n",
    "    'credit_recovery_cleaned', 'dual_enrollment_cleaned', 'enrollment_cleaned', 'expulsions_cleaned',\n",
    "    'geometry_cleaned', 'gifted_and_talented_cleaned', 'harassment_and_bullying_cleaned',\n",
    "    'international_baccalaureate_cleaned', 'justice_facilities_cleaned', 'offenses_cleaned', 'physics_cleaned'\n",
    "]\n",
    "\n",
    "\n",
    "# For each LEAID, find unique school names across the tables\n",
    "for leaid_tuple in missing_leaids[0:100]:\n",
    "    leaid = leaid_tuple[0]\n",
    "    leaids_school_names[leaid] = set()\n",
    "\n",
    "    for table in tables_to_search:\n",
    "        query_get_sch_name = f\"SELECT DISTINCT sch_name FROM {table} WHERE LEAID = '{leaid}';\"\n",
    "        cursor.execute(query_get_sch_name)\n",
    "        sch_names = cursor.fetchall()\n",
    "\n",
    "        for name in sch_names:\n",
    "            leaids_school_names[leaid].add(name[0])\n",
    "\n",
    "# Print the LEAID and its associated unique school names\n",
    "for leaid, school_names in leaids_school_names.items():\n",
    "    print(f\"LEAID: {leaid}, Unique School Names: {', '.join(school_names)}\")\n",
    "\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from SLGA (2017 School Locations & Geoassignments)\n",
    "\n",
    "This data set contains address geocodes (estimated latitude/latitude values) and other geo- graphic indicators to public schools, public local education agencies, private schools, and postsecondary schools. The geographic data are provided as shapefiles, and basic attribute data are available as Excel and SAS tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re  # Import the re module\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = '/Users/abigailiovino/Desktop/Datathon/data/EDGE_GEOCODE_PUBLICLEA_1718/EDGE_GEOCODE_PUBLICLEA_1718.xlsx'\n",
    "geographic_data_df = pd.read_excel(file_path)\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(column_name):\n",
    "    cleaned_name = re.sub(r'\\W+', '_', column_name).strip('_').lower()\n",
    "    return cleaned_name if cleaned_name else 'invalid_column_name'\n",
    "\n",
    "# Function to guess data types for SQL, with special handling for 'id' columns\n",
    "def guess_data_types(df):\n",
    "    column_types = {}\n",
    "    for col in df.columns:\n",
    "        cleaned_col = clean_column_name(col)\n",
    "        unique_values = df[col].dropna().unique()\n",
    "        max_length = df[col].astype(str).map(len).max()\n",
    "\n",
    "        # Handling for 'id' columns or columns with alphanumeric values\n",
    "        if cleaned_col == 'lat' or cleaned_col == 'lon' or  cleaned_col =='loc_other':\n",
    "            column_types[cleaned_col] = 'DECIMAL'\n",
    "        else:\n",
    "            column_types[cleaned_col] = f'VARCHAR({max_length})'\n",
    "    return column_types\n",
    "\n",
    "# Function to read, clean, and return a DataFrame from an Excel file\n",
    "def read_and_clean_excel(file_path, header_row_index=2):\n",
    "    df = pd.read_excel(file_path, header=header_row_index)\n",
    "    df_cleaned = df.dropna(how='all')\n",
    "    return df_cleaned\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "# Establish database connection\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Use the functions defined earlier to create the SQL CREATE TABLE command\n",
    "column_types = guess_data_types(geographic_data_df)\n",
    "print(column_types)\n",
    "columns_sql = ', '.join([f'\"{clean_column_name(col)}\" {col_type}' for col, col_type in column_types.items()])\n",
    "\n",
    "table_name = 'school_geographic_data'  # Name your table\n",
    "try:\n",
    "    # Drop the existing table if it exists\n",
    "    cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\";')\n",
    "    conn.commit()\n",
    "\n",
    "    # Create the table\n",
    "    create_table_sql = f'CREATE TABLE \"{table_name}\" ({columns_sql});'\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "\n",
    "    # Insert data from DataFrame into the table\n",
    "    for index, row in geographic_data_df.iterrows():\n",
    "        columns = ', '.join([f'\"{clean_column_name(col)}\"' for col in row.index])\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        insert_sql = f'INSERT INTO \"{table_name}\" ({columns}) VALUES ({values});'\n",
    "        cursor.execute(insert_sql, tuple(row.values))\n",
    "\n",
    "    conn.commit()  # Commit changes\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    conn.rollback()  # Rollback in case of error\n",
    "\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# Load the Excel file\n",
    "excel_data = pd.read_excel('/Users/abigailiovino/Desktop/Datathon/data/GRF17/grf17_lea_blkgrp.xlsx')\n",
    "\n",
    "# Connect to your PostgreSQL database\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "# Establish database connection\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the table (if not already created)\n",
    "cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS lea_block_groups (\n",
    "        leaid VARCHAR(7),\n",
    "        name_lea17 VARCHAR(100),\n",
    "        blkgrp VARCHAR(12),\n",
    "        count INTEGER,\n",
    "        landarea FLOAT,\n",
    "        waterarea FLOAT\n",
    "    )\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Insert data into the table\n",
    "for idx, row in excel_data.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO lea_block_groups (leaid, name_lea17, blkgrp, count, landarea, waterarea) \n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['LEAID'], row['NAME_LEA17'], row['BLKGRP'], row['COUNT'], row['LANDAREA'], row['WATERAREA']))\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing from ACLF (2020 Address Count Listing Files of the Bureau of the Census )\n",
    "The files include total housing units (including transitory units) and total group quarters counts, by 2020 census tabulation block. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining tables \n",
    "\n",
    "Joining all tables ending in \"_cleaned\" with \"school_poverty_data\" by connecting the leaid's with the \"state_fips_code\" and \"district_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/55_Wisconsin_AddressBlockCountList_062022.txt\n",
      "Creating table aclf_data_national\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/20_Kansas_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/13_Georgia_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/24_Maryland_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/38_NorthDakota_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/02_Alaska_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/54_WestVirginia_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/26_Michigan_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/11_DistrictofColumbia_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/19_Iowa_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/46_SouthDakota_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/12_Florida_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/40_Oklahoma_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/29_Missouri_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/05_Arkansas_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/15_Hawaii_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/27_Minnesota_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/04_Arizona_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/53_Washington_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/39_Ohio_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/21_Kentucky_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/37_NorthCarolina_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/10_Delaware_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/06_California_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/47_Tennessee_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/45_SouthCarolina_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/72_PuertoRico_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/25_Massachusetts_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/01_Alabama_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/23_Maine_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/42_Pennsylvania_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/08_Colorado_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/35_NewMexico_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/34_NewJersey_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/51_Virginia_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/50_Vermont_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/32_Nevada_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/09_Connecticut_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/18_Indiana_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/30_Montana_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/36_NewYork_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/17_Illinois_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/33_NewHampshire_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/41_Oregon_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/44_RhodeIsland_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/22_Louisiana_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/49_Utah_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/31_Nebraska_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/48_Texas_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/16_Idaho_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/56_Wyoming_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n",
      "Importing /Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates/28_Mississippi_AddressBlockCountList_062022.txt\n",
      "TABLE already exists aclf_data_national\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "\n",
    "def clean_column_name(column_name):\n",
    "    cleaned_name = re.sub(r'\\W+', '_', column_name).strip('_').lower()\n",
    "    return cleaned_name if cleaned_name else 'invalid_column_name'\n",
    "\n",
    "def guess_data_types(df):\n",
    "    column_types = {}\n",
    "    for col in df.columns:\n",
    "        cleaned_col = clean_column_name(col)\n",
    "        if 'total' in cleaned_col:\n",
    "            column_types[cleaned_col] = 'INTEGER'\n",
    "        elif cleaned_col in ['state', 'county', 'tract', 'block', 'block_geoid']:\n",
    "            column_types[cleaned_col] = f'VARCHAR({df[col].astype(str).map(len).max()})'\n",
    "        else:\n",
    "            column_types[cleaned_col] = 'DECIMAL'\n",
    "    return column_types\n",
    "\n",
    "def table_exists(cursor, table_name):\n",
    "    cursor.execute(\"select exists(select * from information_schema.tables where table_name=%s)\", (table_name,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "def import_txt_file(file_path, conn_params, table_name='aclf_data_national'):\n",
    "    data_types = {'STATE': str, 'COUNTY': str, 'TRACT': str}\n",
    "    data_df = pd.read_csv(file_path, delimiter='|', dtype=data_types)\n",
    "\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    column_types = guess_data_types(data_df)\n",
    "\n",
    "    if not table_exists(cursor, table_name):\n",
    "        print(\"Creating table\", table_name)\n",
    "        columns_sql = ', '.join([f'\"{clean_column_name(col)}\" {col_type}' for col, col_type in column_types.items()])\n",
    "        create_table_sql = f'CREATE TABLE \"{table_name}\" ({columns_sql});'\n",
    "        cursor.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "    print(\"TABLE already exists\", table_name)\n",
    "\n",
    "    for index, row in data_df.iterrows():\n",
    "        columns = ', '.join([f'\"{clean_column_name(col)}\"' for col in row.index])\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        insert_sql = f'INSERT INTO \"{table_name}\" ({columns}) VALUES ({values});'\n",
    "        cursor.execute(insert_sql, tuple(row.values))\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "def import_all_txt_files(directory, conn_params, table_name='aclf_data_national'):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f'Importing {file_path}')\n",
    "            import_txt_file(file_path, conn_params, table_name)\n",
    "\n",
    "# Directory containing the text files\n",
    "directory = '/Users/abigailiovino/Desktop/Datathon/data/ACLF_AddressCountListingFiles2020_AllStates'\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'\n",
    "}\n",
    "\n",
    "import_all_txt_files(directory, conn_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from the GRF17file of the 2017 School District Geographic Relationship Files (SDGR)\n",
    "\n",
    "National Center for Education Statistics at the Department of Education. Most school districts in the U.S. are independent local governments that have author- ity to determine their geographic boundaries. These boundaries may or may not be consistent with boundaries for other types of legal and statistical areas like counties, Congressional Districts, or Census tracts. As a result, school districts may have multiple spatial associations with other types of geographic areas e.g., a school district boundary may include territory in two different counties, or inter- sect three different Congressional Districts. The NCES EDGE school and agency location files and the NCES Common Core of Data (CCD) provides a limited set of geographic associations based on the location of the district administrative office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          LEAID                              NAME_LEA17        TRACT  COUNT  \\\n",
      "0       0100001             Fort Rucker School District  01031010300      2   \n",
      "1       0100001             Fort Rucker School District  01045020000      2   \n",
      "2       0100003             Maxwell AFB School District  01101000900      3   \n",
      "3       0100003             Maxwell AFB School District  01101001000      3   \n",
      "4       0100003             Maxwell AFB School District  01101006000      3   \n",
      "...         ...                                     ...          ...    ...   \n",
      "113316  7800030  Virgin Islands Department of Education  78030960900     32   \n",
      "113317  7800030  Virgin Islands Department of Education  78030961000     32   \n",
      "113318  7800030  Virgin Islands Department of Education  78030961100     32   \n",
      "113319  7800030  Virgin Islands Department of Education  78030961200     32   \n",
      "113320  7800030  Virgin Islands Department of Education  78030990000     32   \n",
      "\n",
      "         LANDAREA   WATERAREA  \n",
      "0       23.428498    0.000000  \n",
      "1       66.536108    1.056076  \n",
      "2        3.256184    0.218865  \n",
      "3        0.001526    0.000000  \n",
      "4        0.003588    0.000000  \n",
      "...           ...         ...  \n",
      "113316   3.147246    1.173776  \n",
      "113317   0.812847    0.910606  \n",
      "113318   1.356637    0.000000  \n",
      "113319   0.392957    0.309706  \n",
      "113320   0.000000  265.417692  \n",
      "\n",
      "[113321 rows x 6 columns]\n",
      "Distinct number of LEAIDs in the DataFrame: 13331\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import re\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = '/Users/abigailiovino/Desktop/Datathon/data/GRF17/grf17_lea_tract.xlsx'  # Replace with your actual file path\n",
    "\n",
    "# Specify data types for LEAID and TRACT columns to ensure leading zeros are preserved\n",
    "data_types = {\n",
    "    'LEAID': str,\n",
    "    'TRACT': str\n",
    "}\n",
    "\n",
    "# Read the Excel file into a DataFrame with specified data types\n",
    "grf17_lea_tract_df = pd.read_excel(file_path, dtype=data_types)\n",
    "\n",
    "print(grf17_lea_tract_df)\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(column_name):\n",
    "    cleaned_name = re.sub(r'\\W+', '_', column_name).strip('_').lower()\n",
    "    return cleaned_name if cleaned_name else 'invalid_column_name'\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "\n",
    "# Establish database connection\n",
    "conn = psycopg2.connect(**conn_params)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Creating the SQL CREATE TABLE command\n",
    "column_types_sql = {\n",
    "    'LEAID': 'VARCHAR(7)',\n",
    "    'NAME_LEA17': f\"VARCHAR({grf17_lea_tract_df['NAME_LEA17'].astype(str).map(len).max()})\",\n",
    "    'TRACT': 'VARCHAR(11)',\n",
    "    'COUNT': 'INTEGER',\n",
    "    'LANDAREA': 'FLOAT',\n",
    "    'WATERAREA': 'FLOAT'\n",
    "}\n",
    "columns_sql = ', '.join([f'\"{clean_column_name(col)}\" {col_type}' for col, col_type in column_types_sql.items()])\n",
    "table_name = 'school_tract_data'  # Name your table\n",
    "\n",
    "# Create the table in the database\n",
    "try:\n",
    "    cursor.execute(f'DROP TABLE IF EXISTS \"{table_name}\";')\n",
    "    create_table_sql = f'CREATE TABLE \"{table_name}\" ({columns_sql});'\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the table: {e}\")\n",
    "    conn.rollback()\n",
    "\n",
    "# Insert data from DataFrame into the table\n",
    "try:\n",
    "    for index, row in grf17_lea_tract_df.iterrows():\n",
    "        columns = ', '.join([f'\"{clean_column_name(col)}\"' for col in row.index])\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        insert_sql = f'INSERT INTO \"{table_name}\" ({columns}) VALUES ({values});'\n",
    "        cursor.execute(insert_sql, tuple(row))\n",
    "    conn.commit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while inserting data: {e}\")\n",
    "    conn.rollback()\n",
    "\n",
    "# Close the connection\n",
    "finally:\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Print out the distinct number of LEAIDs in the pandas DataFrame\n",
    "distinct_leaids = grf17_lea_tract_df['LEAID'].nunique()\n",
    "print(f\"Distinct number of LEAIDs in the DataFrame: {distinct_leaids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Students_in_Poverty</th>\n",
       "      <th>Total_Students</th>\n",
       "      <th>Sum_Male_Enrolled</th>\n",
       "      <th>Sum_Female_Enrolled</th>\n",
       "      <th>Total_Enrolled</th>\n",
       "      <th>Percent_In_Poverty</th>\n",
       "      <th>Percent_Enrolled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Advanced Mathematics</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>141</td>\n",
       "      <td>182</td>\n",
       "      <td>323</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>7.849332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Algebra II</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>106</td>\n",
       "      <td>109</td>\n",
       "      <td>215</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>5.224787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Biology</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>242</td>\n",
       "      <td>287</td>\n",
       "      <td>529</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>12.855407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>1.215067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>181</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>4.398542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Geometry</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>169</td>\n",
       "      <td>160</td>\n",
       "      <td>329</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>7.995140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Gifted and Talented</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>166</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>4.034022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0100005</td>\n",
       "      <td>Physics</td>\n",
       "      <td>1546</td>\n",
       "      <td>4115</td>\n",
       "      <td>169</td>\n",
       "      <td>113</td>\n",
       "      <td>282</td>\n",
       "      <td>37.569866</td>\n",
       "      <td>6.852977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Advanced Mathematics</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>180</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>2.054325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Algebra II</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>267</td>\n",
       "      <td>221</td>\n",
       "      <td>488</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>5.569505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Biology</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>366</td>\n",
       "      <td>386</td>\n",
       "      <td>752</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>8.582515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>0.524994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>122</td>\n",
       "      <td>132</td>\n",
       "      <td>254</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>2.898882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Geometry</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>338</td>\n",
       "      <td>280</td>\n",
       "      <td>618</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>7.053184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Gifted and Talented</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>1.186944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0100006</td>\n",
       "      <td>Physics</td>\n",
       "      <td>2495</td>\n",
       "      <td>8762</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>28.475234</td>\n",
       "      <td>0.216845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0100007</td>\n",
       "      <td>Advanced Mathematics</td>\n",
       "      <td>1038</td>\n",
       "      <td>14679</td>\n",
       "      <td>751</td>\n",
       "      <td>821</td>\n",
       "      <td>1572</td>\n",
       "      <td>7.071326</td>\n",
       "      <td>10.709176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0100007</td>\n",
       "      <td>Algebra II</td>\n",
       "      <td>1038</td>\n",
       "      <td>14679</td>\n",
       "      <td>154</td>\n",
       "      <td>90</td>\n",
       "      <td>244</td>\n",
       "      <td>7.071326</td>\n",
       "      <td>1.662239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0100007</td>\n",
       "      <td>Biology</td>\n",
       "      <td>1038</td>\n",
       "      <td>14679</td>\n",
       "      <td>694</td>\n",
       "      <td>704</td>\n",
       "      <td>1398</td>\n",
       "      <td>7.071326</td>\n",
       "      <td>9.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0100007</td>\n",
       "      <td>Calculus</td>\n",
       "      <td>1038</td>\n",
       "      <td>14679</td>\n",
       "      <td>184</td>\n",
       "      <td>173</td>\n",
       "      <td>357</td>\n",
       "      <td>7.071326</td>\n",
       "      <td>2.432046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID               Subject  Students_in_Poverty  Total_Students  \\\n",
       "0   0100005  Advanced Mathematics                 1546            4115   \n",
       "1   0100005            Algebra II                 1546            4115   \n",
       "2   0100005               Biology                 1546            4115   \n",
       "3   0100005              Calculus                 1546            4115   \n",
       "4   0100005             Chemistry                 1546            4115   \n",
       "5   0100005              Geometry                 1546            4115   \n",
       "6   0100005   Gifted and Talented                 1546            4115   \n",
       "7   0100005               Physics                 1546            4115   \n",
       "8   0100006  Advanced Mathematics                 2495            8762   \n",
       "9   0100006            Algebra II                 2495            8762   \n",
       "10  0100006               Biology                 2495            8762   \n",
       "11  0100006              Calculus                 2495            8762   \n",
       "12  0100006             Chemistry                 2495            8762   \n",
       "13  0100006              Geometry                 2495            8762   \n",
       "14  0100006   Gifted and Talented                 2495            8762   \n",
       "15  0100006               Physics                 2495            8762   \n",
       "16  0100007  Advanced Mathematics                 1038           14679   \n",
       "17  0100007            Algebra II                 1038           14679   \n",
       "18  0100007               Biology                 1038           14679   \n",
       "19  0100007              Calculus                 1038           14679   \n",
       "\n",
       "    Sum_Male_Enrolled  Sum_Female_Enrolled  Total_Enrolled  \\\n",
       "0                 141                  182             323   \n",
       "1                 106                  109             215   \n",
       "2                 242                  287             529   \n",
       "3                  22                   28              50   \n",
       "4                  87                   94             181   \n",
       "5                 169                  160             329   \n",
       "6                  88                   78             166   \n",
       "7                 169                  113             282   \n",
       "8                  86                   94             180   \n",
       "9                 267                  221             488   \n",
       "10                366                  386             752   \n",
       "11                 30                   16              46   \n",
       "12                122                  132             254   \n",
       "13                338                  280             618   \n",
       "14                 52                   52             104   \n",
       "15                 14                    5              19   \n",
       "16                751                  821            1572   \n",
       "17                154                   90             244   \n",
       "18                694                  704            1398   \n",
       "19                184                  173             357   \n",
       "\n",
       "    Percent_In_Poverty  Percent_Enrolled  \n",
       "0            37.569866          7.849332  \n",
       "1            37.569866          5.224787  \n",
       "2            37.569866         12.855407  \n",
       "3            37.569866          1.215067  \n",
       "4            37.569866          4.398542  \n",
       "5            37.569866          7.995140  \n",
       "6            37.569866          4.034022  \n",
       "7            37.569866          6.852977  \n",
       "8            28.475234          2.054325  \n",
       "9            28.475234          5.569505  \n",
       "10           28.475234          8.582515  \n",
       "11           28.475234          0.524994  \n",
       "12           28.475234          2.898882  \n",
       "13           28.475234          7.053184  \n",
       "14           28.475234          1.186944  \n",
       "15           28.475234          0.216845  \n",
       "16            7.071326         10.709176  \n",
       "17            7.071326          1.662239  \n",
       "18            7.071326          9.523810  \n",
       "19            7.071326          2.432046  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "def get_cleaned_tables(cursor):\n",
    "    \"\"\"Retrieve a list of table names ending with '_cleaned'.\"\"\"\n",
    "    query = \"SELECT tablename FROM pg_tables WHERE tablename LIKE '%_cleaned';\"\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "def join_cleaned_with_poverty_accordingto_id():\n",
    "    # Connect to the PostgreSQL database\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Get the list of '_cleaned' tables\n",
    "        cleaned_tables = get_cleaned_tables(cursor)\n",
    "\n",
    "        # Iterate over the list of tables, construct, and execute the JOIN queries\n",
    "        for table_name in cleaned_tables:\n",
    "            query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM {table_name}\n",
    "            JOIN school_poverty_data\n",
    "            ON {table_name}.leaid = (school_poverty_data.state_fips_code || school_poverty_data.district_id);\n",
    "            \"\"\"\n",
    "            # For demonstration, the query is printed instead of executed\n",
    "            print(f\"SQL Query for {table_name}:\\n{query}\\n\")\n",
    "            # To execute, uncomment the next line and handle the result as needed\n",
    "            # cursor.execute(query)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        # Close the database connection\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "def fetch_poverty_ratio(cursor):\n",
    "    \"\"\"Fetch poverty data including state FIPS code, district ID, estimated_number_of_relevant_children_5_to_17_years_old_in_poverty, estimated_population_5_17\"\"\"\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT state_fips_code, district_id, estimated_number_of_relevant_children_5_to_17_years_old_in_poverty, estimated_population_5_17\n",
    "        FROM school_poverty_data\n",
    "        WHERE estimated_population_5_17 > 1\n",
    "    \"\"\")\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def fetch_school_data(cursor, table_name):\n",
    "    \"\"\"Fetch data from a specific '_cleaned' class table with a JOIN on poverty data.\"\"\"\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT {table_name}.leaid, {table_name}.lea_state_name\n",
    "        FROM {table_name}\n",
    "        JOIN school_poverty_data\n",
    "        ON {table_name}.leaid = (school_poverty_data.state_fips_code || school_poverty_data.district_id);\n",
    "    \"\"\"\n",
    "    cursor.execute(sql_query)\n",
    "    results = cursor.fetchall()\n",
    "    total_schools_fetched = len(results)  # Total number of schools fetched\n",
    "    #print(f\"Query executed: {cursor.query}\")  # For debugging\n",
    "    #print(f\"Number of rows fetched: {total_schools_fetched}\")  # For debugging\n",
    "    \n",
    "    return results, total_schools_fetched\n",
    "\n",
    "        \n",
    "def fetch_poverty_ratio_per_class():\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Fetch poverty data\n",
    "        poverty_data = fetch_poverty_ratio(cursor)\n",
    "\n",
    "        # Names of your \"_cleaned\" tables\n",
    "        cleaned_tables = ['advanced_mathematics_cleaned', 'advanced_placement_cleaned', 'algebra_i_cleaned', 'algebra_ii_cleaned',\n",
    "                          'biology_cleaned', 'calculus_cleaned', 'chemistry_cleaned', 'corporal_punishment_cleaned', 'credit_recovery_cleaned', \n",
    "                          'dual_enrollment_cleaned', 'enrollment_cleaned', 'expulsions_cleaned', 'geometry_cleaned',\n",
    "                          'gifted_and_talented_cleaned','harassment_and_bullying_cleaned', 'international_baccalaureate_cleaned',\n",
    "                          'justice_facilities_cleaned', 'offenses_cleaned', 'physics_cleaned']  # Add other table names as needed\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for table_name in cleaned_tables:\n",
    "            # Fetch school data\n",
    "            school_data, total_schools_surveyed = fetch_school_data(cursor, table_name)\n",
    "            print(f\"Total schools fetched from {table_name}: {total_schools_surveyed}\")\n",
    "            schools_with_high_poverty = 0\n",
    "\n",
    "            for leaid, school_name in school_data:\n",
    "                for state_fips_code, district_id, students_in_poverty, total_students in poverty_data:\n",
    "                    #print(state_fips_code, district_id, students_in_poverty, total_students)\n",
    "                    poverty_ratio = students_in_poverty/total_students\n",
    "                    # Check if LEAID matches and the poverty ratio is above the threshold\n",
    "                    if leaid == state_fips_code + district_id and poverty_ratio>.2:  # Adjust the poverty threshold as needed\n",
    "                        schools_with_high_poverty += 1\n",
    "                        # Print details for each row where the condition is true\n",
    "                        #print(f\"LEAID: {leaid}, Data: {table_name}, School: {school_name}, Poverty Ratio: {poverty_ratio:.2f}\")\n",
    "\n",
    "            results.append({'data_group': table_name, 'num_schools_with_high_poverty': schools_with_high_poverty, 'total_schools_surveyed': total_schools_surveyed})\n",
    "        \n",
    "        for group in results:\n",
    "            print(group)\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "#join_cleaned_with_poverty_accordingto_id()\n",
    "\n",
    "def fetch_poverty_ratio_per_class_optimized():\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "        SELECT t.leaid, t.subject,\n",
    "            p.estimated_number_of_relevant_children_5_to_17_years_old_in_poverty AS children_in_poverty,\n",
    "            p.estimated_population_5_17 AS population_5_17,\n",
    "            t.enrollment_male,\n",
    "            t.enrollment_female\n",
    "        FROM (\n",
    "            SELECT leaid, 'Advanced Mathematics' AS subject,\n",
    "                CASE WHEN \"tot_mathenr_advm_m\" <> '-9' THEN \"tot_mathenr_advm_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_mathenr_advm_f\" <> '-9' THEN \"tot_mathenr_advm_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM advanced_mathematics_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Advanced Placement' AS subject,\n",
    "                CASE WHEN \"tot_apscienr_m\" <> '-9' THEN \"tot_apscienr_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_apscienr_f\" <> '-9' THEN \"tot_apscienr_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM advanced_placement_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Algebra I' AS subject,\n",
    "                CASE WHEN \"tot_algenr_gs1112_m\" <> '-9' THEN \"tot_algenr_gs1112_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_algenr_gs1112_f\" <> '-9' THEN \"tot_algenr_gs1112_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM algebra_i_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Algebra II' AS subject,\n",
    "                CASE WHEN \"tot_mathenr_alg2_m\" <> '-9' THEN \"tot_mathenr_alg2_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_mathenr_alg2_f\" <> '-9' THEN \"tot_mathenr_alg2_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM algebra_ii_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Biology' AS subject,\n",
    "                CASE WHEN \"tot_scienr_biol_m\" <> '-9' THEN \"tot_scienr_biol_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_scienr_biol_f\" <> '-9' THEN \"tot_scienr_biol_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM biology_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Calculus' AS subject,\n",
    "                CASE WHEN \"tot_mathenr_calc_m\" <> '-9' THEN \"tot_mathenr_calc_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_mathenr_calc_f\" <> '-9' THEN \"tot_mathenr_calc_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM calculus_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Chemistry' AS subject,\n",
    "                CASE WHEN \"tot_scienr_chem_m\" <> '-9' THEN \"tot_scienr_chem_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_scienr_chem_f\" <> '-9' THEN \"tot_scienr_chem_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM chemistry_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Geometry' AS subject,\n",
    "                CASE WHEN \"tot_mathenr_geom_m\" <> '-9' THEN \"tot_mathenr_geom_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_mathenr_geom_f\" <> '-9' THEN \"tot_mathenr_geom_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM geometry_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Gifted and Talented' AS subject,\n",
    "                CASE WHEN \"tot_gtenr_m\" <> '-9' THEN \"tot_gtenr_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_gtenr_f\" <> '-9' THEN \"tot_gtenr_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM gifted_and_talented_cleaned\n",
    "            UNION ALL\n",
    "            SELECT leaid, 'Physics' AS subject,\n",
    "                CASE WHEN \"tot_scienr_phys_m\" <> '-9' THEN \"tot_scienr_phys_m\" ELSE NULL END AS enrollment_male,\n",
    "                CASE WHEN \"tot_scienr_phys_f\" <> '-9' THEN \"tot_scienr_phys_f\" ELSE NULL END AS enrollment_female\n",
    "            FROM physics_cleaned\n",
    "        ) t\n",
    "        JOIN school_poverty_data p ON t.leaid = (p.state_fips_code || p.district_id)\n",
    "        WHERE p.estimated_population_5_17 > 0 AND (\n",
    "            t.enrollment_male IS NOT NULL OR \n",
    "            t.enrollment_female IS NOT NULL\n",
    "        )\n",
    "        GROUP BY t.leaid, t.subject, children_in_poverty, population_5_17, enrollment_male, enrollment_female;\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        results = cursor.fetchall()\n",
    "        # Process the results as needed, such as calculating poverty ratios and preparing data for visualization.\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_districts_df(sql_data, columns):\n",
    "    # Convert the fetched data into a pandas DataFrame\n",
    "    df_schools = pd.DataFrame(sql_data, columns=columns)\n",
    "\n",
    "    # each row is a school\n",
    "    df_schools.sort_values(by=['Subject', 'ID'], inplace=True)\n",
    "\n",
    "\n",
    "    # Convert 'Male_Enrolled' and 'Female_Enrolled' columns to numeric, coercing errors to NaN\n",
    "    df_schools['Male_Enrolled'] = pd.to_numeric(df_schools['Male_Enrolled'], errors='coerce')\n",
    "    df_schools['Female_Enrolled'] = pd.to_numeric(df_schools['Female_Enrolled'], errors='coerce')\n",
    "\n",
    "    # Sum male enrolled for each unique combination of ID and Subject\n",
    "    male_sum = df_schools.groupby(['ID', 'Subject', 'Students_in_Poverty', 'Total_Students'])['Male_Enrolled'].sum().reset_index()\n",
    "\n",
    "    # Sum female enrolled for each unique combination of ID and Subject\n",
    "    female_sum = df_schools.groupby(['ID', 'Subject', 'Students_in_Poverty', 'Total_Students'])['Female_Enrolled'].sum().reset_index()\n",
    "\n",
    "    # Merge male_sum and female_sum dataframes on 'ID' and 'Subject' columns\n",
    "    # Now each row is a district\n",
    "    df_districts = pd.merge(male_sum, female_sum, on=['ID', 'Subject', 'Students_in_Poverty', 'Total_Students'])\n",
    "\n",
    "    # Rename columns to reflect the sum of male and female enrolled\n",
    "    df_districts.rename(columns={'Male_Enrolled': 'Sum_Male_Enrolled', 'Female_Enrolled': 'Sum_Female_Enrolled'}, inplace=True)\n",
    "\n",
    "    # Add an extra column with the total enrolled (male + female)\n",
    "    df_districts['Total_Enrolled'] = df_districts['Sum_Male_Enrolled'] + df_districts['Sum_Female_Enrolled']\n",
    "\n",
    "    # Ensure the appropriate columns are numeric\n",
    "    df_districts['Percent_In_Poverty'] = (df_districts['Students_in_Poverty'] / df_districts['Total_Students']) * 100\n",
    "    df_districts['Percent_Enrolled'] = (df_districts['Total_Enrolled'] / df_districts['Total_Students']) * 100\n",
    "\n",
    "    return df_districts\n",
    "    \n",
    "\n",
    "sql_data = fetch_poverty_ratio_per_class_optimized()\n",
    "columns = ['ID', 'Subject', 'Students_in_Poverty', 'Total_Students', 'Male_Enrolled', 'Female_Enrolled']\n",
    "    \n",
    "df_districts = create_districts_df(sql_data, columns)\n",
    "\n",
    "# Displaying the resulting dataframe\n",
    "df_districts.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only look at schools that offer that subject.\n",
    "\n",
    "When graphing, continuous lines are easier to read than bars because of the large number of districts. \n",
    "\n",
    "To display continuous lines,  we need to ensure that the ID is treated as a continuous variable. However, since the ID seems to be a categorical variable (such as a school or district code), we need to convert it into a numeric sequence that represents the order after sorting by poverty percentage. This sequence will serve as our x-axis values, providing a continuous line when plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    }
   ],
   "source": [
    "# Load R\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 10  7\n",
      "   Subject  Mean_Poverty Median_Poverty SD_Poverty Mean_Enrolled Median_Enrolled\n",
      "   <chr>           <dbl>          <dbl>      <dbl>         <dbl>           <dbl>\n",
      " 1 Advance         16.5           15.2       9.13         4.83            3.82 \n",
      " 2 Advance         14.8           13.3       8.88         2.21            1.37 \n",
      " 3 Algebra         16.6           15.3       9.05         0.502           0.132\n",
      " 4 Algebra         16.8           15.5       9.22         6.73            5.71 \n",
      " 5 Biology          16.8           15.5       9.21         9.49            8.01 \n",
      " 6 Calculus         15.3           14.0       8.56         1.68            1.11 \n",
      " 7 Chemist         16.6           15.3       9.13         6.03            5.04 \n",
      " 8 Geometry         16.9           15.6       9.20         7.32            6.53 \n",
      " 9 Gifted          17.2           16.0       9.35         8.49            5.53 \n",
      "10 Physics          15.9           14.6       8.80         4.21            2.46 \n",
      "#  1 more variable: SD_Enrolled <dbl>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "In addition: Warning messages:\n",
       "1: Removed 9 rows containing missing values (`geom_point()`). \n",
       "2: Removed 12 rows containing missing values (`geom_point()`). \n",
       "3: Removed 6 rows containing missing values (`geom_point()`). \n",
       "4: Removed 9 rows containing missing values (`geom_point()`). \n",
       "5: Removed 14 rows containing missing values (`geom_point()`). \n",
       "6: Removed 5 rows containing missing values (`geom_point()`). \n",
       "7: Removed 4 rows containing missing values (`geom_point()`). \n",
       "8: Removed 2 rows containing missing values (`geom_point()`). \n",
       "9: Removed 4 rows containing missing values (`geom_point()`). \n",
       "10: Removed 5 rows containing missing values (`geom_point()`). \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i df_districts\n",
    "\n",
    "# Load necessary libraries\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(tidyr)  # For pivot_longer()\n",
    "library(scales)\n",
    "library(gridExtra)\n",
    "\n",
    "\n",
    "  # Descriptive statistics for each subject\n",
    "df_summary <- df_districts %>%\n",
    "  group_by(Subject) %>%\n",
    "  summarise(Mean_Poverty = mean(Percent_In_Poverty, na.rm = TRUE),\n",
    "            Median_Poverty = median(Percent_In_Poverty, na.rm = TRUE),\n",
    "            SD_Poverty = sd(Percent_In_Poverty, na.rm = TRUE),\n",
    "            Mean_Enrolled = mean(Percent_Enrolled, na.rm = TRUE),\n",
    "            Median_Enrolled = median(Percent_Enrolled, na.rm = TRUE),\n",
    "            SD_Enrolled = sd(Percent_Enrolled, na.rm = TRUE))\n",
    "\n",
    "print(df_summary)\n",
    "\n",
    "# Arrange the dataframe by Percent_In_Poverty, then by Percent_Enrolled\n",
    "df_districts <- df_districts %>%\n",
    "  arrange(Percent_In_Poverty, Percent_Enrolled) %>%\n",
    "  mutate(District_Order = row_number())\n",
    "\n",
    "# Reshape data to long format for plotting\n",
    "df_long <- df_districts %>%\n",
    "  select(District_Order, Subject, Percent_In_Poverty, Percent_Enrolled) %>%\n",
    "  pivot_longer(cols = c(Percent_In_Poverty, Percent_Enrolled), names_to = \"Metric\", values_to = \"Value\")\n",
    "\n",
    "\n",
    "# Find the maximum value across both Proportion_in_Poverty and Proportion_Enrolled\n",
    "y_axis_limit <- min(max(df_districts$Percent_In_Poverty, df_districts$Percent_Enrolled, na.rm = TRUE), 100)\n",
    "\n",
    "# Custom transformation: compression ratio\n",
    "compression_ratio <- 8\n",
    "\n",
    "# Calculate the 60th percentile for the combined percentages\n",
    "percentile <- quantile(c(df_districts$Percent_Enrolled), probs = 0.75)\n",
    "\n",
    "# Create custom transformation functions\n",
    "# Create custom transformation functions using the 60th percentile\n",
    "my_trans <- trans_new(\n",
    "  name = \"compressed\",\n",
    "  transform = function(x) ifelse(x > percentile, percentile + (x - percentile) / compression_ratio, x),\n",
    "  inverse = function(x) ifelse(x > percentile, percentile + (x - percentile) * compression_ratio, x)\n",
    ")\n",
    "\n",
    "# Create a list to store individual plots\n",
    "plots_list <- list()\n",
    "\n",
    "# Loop through each unique subject and create an individual plot\n",
    "for(subj in unique(df_districts$Subject)) {\n",
    "  df_subset <- df_long %>% filter(Subject == subj)\n",
    "  \n",
    "  p <- ggplot(df_subset, aes(x = District_Order, y = Value, color = Metric, group = Metric)) +\n",
    "    geom_line(data = df_subset %>% filter(Metric == \"Percent_In_Poverty\")) +\n",
    "    geom_point(data = df_subset %>% filter(Metric == \"Percent_Enrolled\"), size = 1, alpha = .3) +\n",
    "    theme_minimal() +\n",
    "    labs(title = subj, y = \"Percentage\") +\n",
    "    theme(\n",
    "      axis.title.x = element_blank(),\n",
    "      axis.text.x = element_blank(),\n",
    "      axis.ticks.x = element_blank(),\n",
    "      legend.position = \"bottom\"\n",
    "    ) +\n",
    "    scale_color_manual(values = setNames(nice_colors, c(\"Percent_In_Poverty\", \"Percent_Enrolled\"))) +\n",
    "    scale_y_continuous(\n",
    "      trans = my_trans, \n",
    "      labels = function(x) paste0(x, \"%\"), \n",
    "      limits = c(0, 100) # Control the limits of the transformed scale\n",
    "    )\n",
    "  \n",
    "  # Add the plot to the list\n",
    "  plots_list[[subj]] <- p\n",
    "}\n",
    "\n",
    "# Create the combined plot object with marrangeGrob\n",
    "# Setting nrow = 2 for two plots per page, and ncol = 1 for a single column of plots\n",
    "combined_plot <- marrangeGrob(plots_list, nrow = 1, ncol = 1)\n",
    "\n",
    "# Standard US Letter dimensions for a portrait orientation\n",
    "page_height <- 8  # in inches\n",
    "page_width <- 10  # in inches\n",
    "\n",
    "# Save the combined plot as a PDF, specifying the entire page size\n",
    "# Since marrangeGrob will handle the arrangement of 2 plots per page, we specify the full page size here\n",
    "ggsave(\"percent_enrolled_and_percent_in_poverty_in_school_district_by_subject.pdf\", \n",
    "       plot = combined_plot, \n",
    "       device = \"pdf\", \n",
    "       width = page_width, \n",
    "       height = page_height,\n",
    "       limitsize = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the correlation between 'Percent Enrolled' and 'Percent in Poverty' for each subject.\n",
    "\n",
    "First, we compute Pearson's correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$`Algebra II`\n",
      "[1] 0.008756442\n",
      "\n",
      "$Biology\n",
      "[1] -0.001431733\n",
      "\n",
      "$Chemistry\n",
      "[1] -0.01825015\n",
      "\n",
      "$Geometry\n",
      "[1] 0.001406277\n",
      "\n",
      "$`Gifted and Talented`\n",
      "[1] 0.007470269\n",
      "\n",
      "$`Algebra I`\n",
      "[1] 0.02855765\n",
      "\n",
      "$`Advanced Mathematics`\n",
      "[1] -0.04762702\n",
      "\n",
      "$Calculus\n",
      "[1] -0.2743764\n",
      "\n",
      "$`Advanced Placement`\n",
      "[1] -0.009374726\n",
      "\n",
      "$Physics\n",
      "[1] 0.00670542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# Load necessary libraries\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(tidyr)  # For pivot_longer()\n",
    "library(scales)\n",
    "\n",
    "# Arrange the dataframe by Percent_In_Poverty, then by Percent_Enrolled\n",
    "df_districts <- df_districts %>%\n",
    "  arrange(Percent_In_Poverty, Percent_Enrolled) %>%\n",
    "  mutate(District_Order = row_number())\n",
    "\n",
    "# Loop through each unique subject and calculate correlation\n",
    "correlation_list <- list()\n",
    "\n",
    "for(subj in unique(df_districts$Subject)) {\n",
    "  df_subset <- df_districts %>% filter(Subject == subj)\n",
    "  \n",
    "  # Calculate Pearson's correlation coefficient\n",
    "  correlation <- cor(df_subset$Percent_In_Poverty, df_subset$Percent_Enrolled, use = \"complete.obs\")\n",
    "  \n",
    "  # Add the correlation to the list with subject as the name\n",
    "  correlation_list[[subj]] <- correlation\n",
    "}\n",
    "\n",
    "# Print the correlations for each subject\n",
    "print(correlation_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the correlation coefficients indicate that there's generally a very weak to negligible linear relationship between 'Percent Enrolled' and 'Percent in Poverty' across the subjects. The strongest (yet still moderate) correlation is observed in Calculus, where there's a slight tendency for districts with higher percentages in poverty to have lower percentages of students enrolled in Calculus. However, these correlations are quite weak, implying that other factors not captured by these two variables might be influencing enrollment percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$`Algebra II`\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -7.20   -2.51   -0.99    0.63 3126.12 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         6.22502    0.63831   9.752   <2e-16 ***\n",
      "Percent_In_Poverty  0.02973    0.03325   0.894    0.371    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 31.3 on 10425 degrees of freedom\n",
      "Multiple R-squared:  7.668e-05,\tAdjusted R-squared:  -1.924e-05 \n",
      "F-statistic: 0.7994 on 1 and 10425 DF,  p-value: 0.3713\n",
      "\n",
      "\n",
      "$Biology\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -9.65   -3.10   -1.48    0.77 1423.88 \n",
      "\n",
      "Coefficients:\n",
      "                    Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         9.535077   0.337197  28.278   <2e-16 ***\n",
      "Percent_In_Poverty -0.002584   0.017593  -0.147    0.883    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 16.61 on 10522 degrees of freedom\n",
      "Multiple R-squared:  2.05e-06,\tAdjusted R-squared:  -9.299e-05 \n",
      "F-statistic: 0.02157 on 1 and 10522 DF,  p-value: 0.8832\n",
      "\n",
      "\n",
      "$Chemistry\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -6.37   -2.57   -1.01    0.96 1527.84 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         6.56786    0.33209  19.778   <2e-16 ***\n",
      "Percent_In_Poverty -0.03214    0.01750  -1.837   0.0663 .  \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 16.07 on 10123 degrees of freedom\n",
      "Multiple R-squared:  0.0003331,\tAdjusted R-squared:  0.0002343 \n",
      "F-statistic: 3.373 on 1 and 10123 DF,  p-value: 0.06631\n",
      "\n",
      "\n",
      "$Geometry\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -7.47   -2.20   -0.79    0.64 1092.65 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)        7.289772   0.250795  29.067   <2e-16 ***\n",
      "Percent_In_Poverty 0.001851   0.013027   0.142    0.887    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 12.1 on 10207 degrees of freedom\n",
      "Multiple R-squared:  1.978e-06,\tAdjusted R-squared:  -9.599e-05 \n",
      "F-statistic: 0.02019 on 1 and 10207 DF,  p-value: 0.887\n",
      "\n",
      "\n",
      "$`Gifted and Talented`\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "   Min     1Q Median     3Q    Max \n",
      " -10.1   -5.8   -3.0    1.2 4290.9 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         7.81504    1.14671   6.815 1.01e-11 ***\n",
      "Percent_In_Poverty  0.03929    0.05855   0.671    0.502    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 49.16 on 8066 degrees of freedom\n",
      "Multiple R-squared:  5.58e-05,\tAdjusted R-squared:  -6.817e-05 \n",
      "F-statistic: 0.4501 on 1 and 8066 DF,  p-value: 0.5023\n",
      "\n",
      "\n",
      "$`Algebra I`\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      " -1.017  -0.472  -0.358  -0.091 176.511 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)        0.380139   0.050333   7.552 4.67e-14 ***\n",
      "Percent_In_Poverty 0.007353   0.002661   2.763  0.00574 ** \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 2.328 on 9353 degrees of freedom\n",
      "Multiple R-squared:  0.0008155,\tAdjusted R-squared:  0.0007087 \n",
      "F-statistic: 7.634 on 1 and 9353 DF,  p-value: 0.005739\n",
      "\n",
      "\n",
      "$`Advanced Mathematics`\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -5.41   -2.48   -0.96    1.05 1029.50 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         5.80384    0.24015  24.168  < 2e-16 ***\n",
      "Percent_In_Poverty -0.05926    0.01274  -4.653 3.31e-06 ***\n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 11.35 on 9524 degrees of freedom\n",
      "Multiple R-squared:  0.002268,\tAdjusted R-squared:  0.002164 \n",
      "F-statistic: 21.65 on 1 and 9524 DF,  p-value: 3.312e-06\n",
      "\n",
      "\n",
      "$Calculus\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "   Min     1Q Median     3Q    Max \n",
      "-2.258 -0.989 -0.468  0.334 39.412 \n",
      "\n",
      "Coefficients:\n",
      "                    Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         2.637927   0.043957   60.01   <2e-16 ***\n",
      "Percent_In_Poverty -0.062750   0.002502  -25.08   <2e-16 ***\n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 1.883 on 7724 degrees of freedom\n",
      "Multiple R-squared:  0.07528,\tAdjusted R-squared:  0.07516 \n",
      "F-statistic: 628.8 on 1 and 7724 DF,  p-value: < 2.2e-16\n",
      "\n",
      "\n",
      "$`Advanced Placement`\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -2.80   -1.46   -0.83    0.16 1164.78 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         2.48032    0.47792   5.190 2.19e-07 ***\n",
      "Percent_In_Poverty -0.01795    0.02769  -0.648    0.517    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 16.99 on 4780 degrees of freedom\n",
      "Multiple R-squared:  8.789e-05,\tAdjusted R-squared:  -0.0001213 \n",
      "F-statistic: 0.4201 on 1 and 4780 DF,  p-value: 0.5169\n",
      "\n",
      "\n",
      "$Physics\n",
      "\n",
      "Call:\n",
      "lm(formula = Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
      "\n",
      "Residuals:\n",
      "    Min      1Q  Median      3Q     Max \n",
      "  -4.87   -3.09   -1.71    1.46 2328.79 \n",
      "\n",
      "Coefficients:\n",
      "                   Estimate Std. Error t value Pr(>|t|)    \n",
      "(Intercept)         3.90461    0.55482   7.038  2.1e-12 ***\n",
      "Percent_In_Poverty  0.01923    0.03049   0.631    0.528    \n",
      "---\n",
      "Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1\n",
      "\n",
      "Residual standard error: 25.24 on 8850 degrees of freedom\n",
      "Multiple R-squared:  4.496e-05,\tAdjusted R-squared:  -6.803e-05 \n",
      "F-statistic: 0.3979 on 1 and 8850 DF,  p-value: 0.5282\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "regression_results <- list()\n",
    "\n",
    "for(subj in unique(df_districts$Subject)) {\n",
    "  df_subset <- df_districts %>% filter(Subject == subj)\n",
    "  \n",
    "  lm_model <- lm(Percent_Enrolled ~ Percent_In_Poverty, data = df_subset)\n",
    "  regression_results[[subj]] <- summary(lm_model)\n",
    "}\n",
    "\n",
    "# Print regression summaries for each subject\n",
    "print(regression_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algebra II, Biology, Chemistry, Geometry, Gifted and Talented, Advanced Placement, Physics: The relationships between 'Percent in Poverty' and 'Percent Enrolled' are not statistically significant as indicated by the high p-values for the slope coefficients.\n",
    "Algebra I, Advanced Mathematics, Calculus: These subjects show a statistically significant relationship between 'Percent in Poverty' and 'Percent Enrolled'. For Algebra I and Advanced Mathematics, the relationship is relatively weak as indicated by the low R-squared values. However, for Calculus, there's a moderate negative relationship, indicating that as poverty increases, enrollment in Calculus tends to decrease.\n",
    "\n",
    "Interpretation of the above: The p-value associated with each coefficient tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) suggests that the effect is statistically significant.\n",
    "For example, in Calculus, the slope has a p-value < 2e-16, indicating a highly significant negative relationship between 'Percent in Poverty' and 'Percent Enrolled'. A one percentage point increase in 'Percent in Poverty' is associated with a 0.06% decrease in 'Percent Enrolled'.\n",
    "\n",
    "The F-statistic tests whether at least one predictor variable has a non-zero coefficient. A low p-value here indicates that the model is significant.\n",
    "For example, the model for Calculus has an F-statistic of 628.8 with a p-value < 2.2e-16, suggesting that the model is highly significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:220\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     result \u001b[39m=\u001b[39m func(left, right)\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[39m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[39mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 149\u001b[0m\n\u001b[1;32m    146\u001b[0m sql_data \u001b[39m=\u001b[39m join_cleaned_with_school_tract_accordingto_id(conn_params)\n\u001b[1;32m    147\u001b[0m columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSubject\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTract\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mStudents_in_Poverty\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTotal_Students\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMale_Enrolled\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFemale_Enrolled\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 149\u001b[0m df_districts \u001b[39m=\u001b[39m create_districts_df(sql_data, columns)\n\u001b[1;32m    151\u001b[0m \u001b[39m# Displaying the resulting dataframe\u001b[39;00m\n\u001b[1;32m    152\u001b[0m df_districts\u001b[39m.\u001b[39mhead(\u001b[39m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 140\u001b[0m, in \u001b[0;36mcreate_districts_df\u001b[0;34m(sql_data, columns)\u001b[0m\n\u001b[1;32m    137\u001b[0m df_districts[\u001b[39m'\u001b[39m\u001b[39mTotal_Enrolled\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_districts[\u001b[39m'\u001b[39m\u001b[39mSum_Male_Enrolled\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m df_districts[\u001b[39m'\u001b[39m\u001b[39mSum_Female_Enrolled\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    139\u001b[0m \u001b[39m# Ensure the appropriate columns are numeric\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m df_districts[\u001b[39m'\u001b[39m\u001b[39mPercent_In_Poverty\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (df_districts[\u001b[39m'\u001b[39;49m\u001b[39mStudents_in_Poverty\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m/\u001b[39;49m df_districts[\u001b[39m'\u001b[39;49m\u001b[39mTotal_Students\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    141\u001b[0m df_districts[\u001b[39m'\u001b[39m\u001b[39mPercent_Enrolled\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (df_districts[\u001b[39m'\u001b[39m\u001b[39mTotal_Enrolled\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m/\u001b[39m df_districts[\u001b[39m'\u001b[39m\u001b[39mTotal_Students\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m df_districts\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/arraylike.py:210\u001b[0m, in \u001b[0;36mOpsMixin.__truediv__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__truediv__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__truediv__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_arith_method(other, operator\u001b[39m.\u001b[39;49mtruediv)\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/series.py:5820\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5818\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   5819\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 5820\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39;49mIndexOpsMixin\u001b[39m.\u001b[39;49m_arith_method(\u001b[39mself\u001b[39;49m, other, op)\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/base.py:1381\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1378\u001b[0m     rvalues \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(rvalues\u001b[39m.\u001b[39mstart, rvalues\u001b[39m.\u001b[39mstop, rvalues\u001b[39m.\u001b[39mstep)\n\u001b[1;32m   1380\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1381\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m   1383\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:285\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    281\u001b[0m     _bool_arith_check(op, left, right)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    283\u001b[0m     \u001b[39m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 285\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:229\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_cmp \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    223\u001b[0m         left\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mgetattr\u001b[39m(right, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m\n\u001b[1;32m    224\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[39m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[39m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m         result \u001b[39m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[1;32m    230\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/penv/venn/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:165\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 165\u001b[0m         result[mask] \u001b[39m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[1;32m    167\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "conn_params = {\n",
    "    \"dbname\": '2017-18_Civil_Rights_Data_Collection',\n",
    "    \"user\": 'postgres',  \n",
    "    \"password\": '',  \n",
    "    \"host\": '127.0.0.1',\n",
    "    \"port\": '5432'  # default PostgreSQL port is 5432\n",
    "}\n",
    "\n",
    "def get_cleaned_tables(cursor):\n",
    "    \"\"\"Retrieve a list of table names ending with '_cleaned'.\"\"\"\n",
    "    query = \"SELECT tablename FROM pg_tables WHERE tablename LIKE '%_cleaned';\"\n",
    "    cursor.execute(query)\n",
    "    return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "def join_cleaned_with_school_tract_accordingto_id(conn_params):\n",
    "    # Connect to the PostgreSQL database\n",
    "    conn = psycopg2.connect(**conn_params)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "\n",
    "    try:\n",
    "            query = \"\"\"\n",
    "            SELECT t.leaid, t.subject,\n",
    "                p.estimated_number_of_relevant_children_5_to_17_years_old_in_poverty AS children_in_poverty,\n",
    "                p.estimated_population_5_17 AS population_5_17,\n",
    "                t.enrollment_male,\n",
    "                t.enrollment_female,\n",
    "                a.tract\n",
    "            FROM (\n",
    "                SELECT leaid, 'Advanced Mathematics' AS subject,\n",
    "                    CASE WHEN \"tot_mathenr_advm_m\" <> '-9' THEN \"tot_mathenr_advm_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_mathenr_advm_f\" <> '-9' THEN \"tot_mathenr_advm_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM advanced_mathematics_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Advanced Placement' AS subject,\n",
    "                    CASE WHEN \"tot_apscienr_m\" <> '-9' THEN \"tot_apscienr_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_apscienr_f\" <> '-9' THEN \"tot_apscienr_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM advanced_placement_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Algebra I' AS subject,\n",
    "                    CASE WHEN \"tot_algenr_gs1112_m\" <> '-9' THEN \"tot_algenr_gs1112_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_algenr_gs1112_f\" <> '-9' THEN \"tot_algenr_gs1112_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM algebra_i_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Algebra II' AS subject,\n",
    "                    CASE WHEN \"tot_mathenr_alg2_m\" <> '-9' THEN \"tot_mathenr_alg2_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_mathenr_alg2_f\" <> '-9' THEN \"tot_mathenr_alg2_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM algebra_ii_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Biology' AS subject,\n",
    "                    CASE WHEN \"tot_scienr_biol_m\" <> '-9' THEN \"tot_scienr_biol_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_scienr_biol_f\" <> '-9' THEN \"tot_scienr_biol_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM biology_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Calculus' AS subject,\n",
    "                    CASE WHEN \"tot_mathenr_calc_m\" <> '-9' THEN \"tot_mathenr_calc_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_mathenr_calc_f\" <> '-9' THEN \"tot_mathenr_calc_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM calculus_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Chemistry' AS subject,\n",
    "                    CASE WHEN \"tot_scienr_chem_m\" <> '-9' THEN \"tot_scienr_chem_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_scienr_chem_f\" <> '-9' THEN \"tot_scienr_chem_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM chemistry_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Geometry' AS subject,\n",
    "                    CASE WHEN \"tot_mathenr_geom_m\" <> '-9' THEN \"tot_mathenr_geom_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_mathenr_geom_f\" <> '-9' THEN \"tot_mathenr_geom_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM geometry_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Gifted and Talented' AS subject,\n",
    "                    CASE WHEN \"tot_gtenr_m\" <> '-9' THEN \"tot_gtenr_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_gtenr_f\" <> '-9' THEN \"tot_gtenr_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM gifted_and_talented_cleaned\n",
    "                UNION ALL\n",
    "                SELECT leaid, 'Physics' AS subject,\n",
    "                    CASE WHEN \"tot_scienr_phys_m\" <> '-9' THEN \"tot_scienr_phys_m\" ELSE NULL END AS enrollment_male,\n",
    "                    CASE WHEN \"tot_scienr_phys_f\" <> '-9' THEN \"tot_scienr_phys_f\" ELSE NULL END AS enrollment_female\n",
    "                FROM physics_cleaned\n",
    "            ) t\n",
    "            JOIN school_poverty_data p ON t.leaid = (p.state_fips_code || p.district_id)\n",
    "            JOIN school_tract_data s ON t.leaid = s.leaid\n",
    "            JOIN aclf_data a ON (SUBSTRING(s.tract FROM 6 FOR 4) || '.' || SUBSTRING(s.tract FROM 10 FOR 2)) = a.tract\n",
    "            WHERE p.estimated_population_5_17 > 0 AND (\n",
    "                t.enrollment_male IS NOT NULL OR \n",
    "                t.enrollment_female IS NOT NULL\n",
    "            )\n",
    "            GROUP BY t.leaid, t.subject, a.tract, p.estimated_number_of_relevant_children_5_to_17_years_old_in_poverty, \n",
    "            p.estimated_population_5_17, enrollment_male, enrollment_female;\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "\n",
    "            results = cursor.fetchall()\n",
    "        # Process the results as needed, such as calculating poverty ratios and preparing data for visualization.\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_districts_df(sql_data, columns):\n",
    "    # Convert the fetched data into a pandas DataFrame\n",
    "    df_schools = pd.DataFrame(sql_data, columns=columns)\n",
    "\n",
    "    # each row is a school\n",
    "    df_schools.sort_values(by=['Subject', 'ID'], inplace=True)\n",
    "\n",
    "\n",
    "    # Convert 'Male_Enrolled' and 'Female_Enrolled' columns to numeric, coercing errors to NaN\n",
    "    df_schools['Male_Enrolled'] = pd.to_numeric(df_schools['Male_Enrolled'], errors='coerce')\n",
    "    df_schools['Female_Enrolled'] = pd.to_numeric(df_schools['Female_Enrolled'], errors='coerce')\n",
    "\n",
    "    # Sum male enrolled for each unique combination of ID and Subject\n",
    "    male_sum = df_schools.groupby(['ID', 'Subject', 'Tract', 'Students_in_Poverty', 'Total_Students'])['Male_Enrolled'].sum().reset_index()\n",
    "\n",
    "    # Sum female enrolled for each unique combination of ID and Subject\n",
    "    female_sum = df_schools.groupby(['ID', 'Subject', 'Tract', 'Students_in_Poverty', 'Total_Students'])['Female_Enrolled'].sum().reset_index()\n",
    "\n",
    "    # Merge male_sum and female_sum dataframes on 'ID' and 'Subject' columns\n",
    "    # Now each row is a district\n",
    "    df_districts = pd.merge(male_sum, female_sum, on=['ID', 'Subject', 'Tract', 'Students_in_Poverty', 'Total_Students'])\n",
    "\n",
    "    # Rename columns to reflect the sum of male and female enrolled\n",
    "    df_districts.rename(columns={'Male_Enrolled': 'Sum_Male_Enrolled', 'Female_Enrolled': 'Sum_Female_Enrolled'}, inplace=True)\n",
    "\n",
    "    # Add an extra column with the total enrolled (male + female)\n",
    "    df_districts['Total_Enrolled'] = df_districts['Sum_Male_Enrolled'] + df_districts['Sum_Female_Enrolled']\n",
    "\n",
    "    # Ensure the appropriate columns are numeric\n",
    "    df_districts['Percent_In_Poverty'] = (df_districts['Students_in_Poverty'] / df_districts['Total_Students']) * 100\n",
    "    df_districts['Percent_Enrolled'] = (df_districts['Total_Enrolled'] / df_districts['Total_Students']) * 100\n",
    "\n",
    "    return df_districts\n",
    "    \n",
    "\n",
    "sql_data = join_cleaned_with_school_tract_accordingto_id(conn_params)\n",
    "columns = ['ID', 'Subject', 'Tract', 'Students_in_Poverty', 'Total_Students', 'Male_Enrolled', 'Female_Enrolled']\n",
    "    \n",
    "df_districts = create_districts_df(sql_data, columns)\n",
    "\n",
    "# Displaying the resulting dataframe\n",
    "df_districts.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data from the 2017 Home Mortgage Disclosure Act (HMDA) database\n",
    "\n",
    "We need to handle a very large CSV, \"hmda_2017_nationwide_all-records_labels.csv\" ,without opening it. W use Python to read the first few lines to inspect its structure. This approach allows us to understand the data's format, column headers, and how to construct our queries without loading the entire file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to /Users/abigailiovino/Desktop/Datathon/data/hmda_2017_nationwide_all-records_labels_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "file_path = '/Users/abigailiovino/Desktop/Datathon/data/hmda_2017_nationwide_all-records_labels.csv'\n",
    "\n",
    "\n",
    "# Define the path for the cleaned CSV file\n",
    "cleaned_file_path = '/Users/abigailiovino/Desktop/Datathon/data/hmda_2017_nationwide_all-records_labels_cleaned.csv'\n",
    "\n",
    "# Specify data types for columns explicitly to avoid dtype mismatch errors\n",
    "dtype = {\n",
    "    'as_of_year': 'int64',\n",
    "    'respondent_id': 'object',\n",
    "    'agency_name': 'object',\n",
    "    'agency_abbr': 'object',\n",
    "    'agency_code': 'object',\n",
    "    'loan_type_name': 'object',\n",
    "    'loan_type': 'object',\n",
    "    'property_type_name': 'object',\n",
    "    'property_type': 'object',\n",
    "    'loan_purpose_name': 'object',\n",
    "    'loan_purpose': 'float64',  # NULLs expected\n",
    "    'owner_occupancy_name': 'object',\n",
    "    'owner_occupancy': 'int64',\n",
    "    'loan_amount_000s': 'float64',\n",
    "    'preapproval_name': 'object',\n",
    "    'preapproval': 'int64',\n",
    "    'action_taken_name': 'object',\n",
    "    'action_taken': 'float64',  # NULLs expected\n",
    "    'msamd_name': 'object',\n",
    "    'msamd': 'object',\n",
    "    'state_name': 'object',\n",
    "    'state_abbr': 'object',\n",
    "    'state_code': 'object',\n",
    "    'county_name': 'object',\n",
    "    'county_code': 'object',\n",
    "    'census_tract_number': 'object',\n",
    "    'applicant_ethnicity_name': 'object',\n",
    "    'applicant_ethnicity': 'float64',  # NULLs expected\n",
    "    'co_applicant_ethnicity_name': 'object',\n",
    "    'co_applicant_ethnicity': 'float64',  # NULLs expected\n",
    "    'applicant_race_name_1': 'object',\n",
    "    'applicant_race_1': 'float64',  # NULLs expected\n",
    "    'applicant_race_name_2': 'object',\n",
    "    'applicant_race_2': 'float64',  # NULLs expected\n",
    "    'applicant_race_name_3': 'object',\n",
    "    'applicant_race_3': 'float64',  # NULLs expected\n",
    "    'applicant_race_name_4': 'object',\n",
    "    'applicant_race_4': 'float64',  # NULLs expected\n",
    "    'applicant_race_name_5': 'object',\n",
    "    'applicant_race_5': 'float64',  # NULLs expected\n",
    "    'co_applicant_race_name_1': 'object',\n",
    "    'co_applicant_race_1': 'float64',  # NULLs expected\n",
    "    'co_applicant_race_name_2': 'object',\n",
    "    'co_applicant_race_2': 'float64',  # NULLs expected\n",
    "    'co_applicant_race_name_3': 'object',\n",
    "    'co_applicant_race_3': 'float64',  # NULLs expected\n",
    "    'co_applicant_race_name_4': 'object',\n",
    "    'co_applicant_race_4': 'float64',  # NULLs expected\n",
    "    'co_applicant_race_name_5': 'object',\n",
    "    'co_applicant_race_5': 'float64',  # NULLs expected\n",
    "    'applicant_sex_name': 'object',\n",
    "    'applicant_sex': 'float64',  # NULLs expected\n",
    "    'co_applicant_sex_name': 'object',\n",
    "    'co_applicant_sex': 'object',\n",
    "    'applicant_income_000s': 'float64',  # NULLs expected\n",
    "    'purchaser_type_name': 'object',\n",
    "    'purchaser_type': 'float64',  # NULLs expected\n",
    "    'denial_reason_name_1': 'object',\n",
    "    'denial_reason_1': 'float64',  # NULLs expected\n",
    "    'denial_reason_name_2': 'object',\n",
    "    'denial_reason_2': 'float64',  # NULLs expected\n",
    "    'denial_reason_name_3': 'object',\n",
    "    'denial_reason_3': 'float64',  # NULLs expected\n",
    "    'rate_spread': 'float64',  # NULLs expected\n",
    "    'hoepa_status_name': 'object',\n",
    "    'hoepa_status': 'float64',  # NULLs expected\n",
    "    'lien_status_name': 'object',\n",
    "    'lien_status': 'object',\n",
    "    'edit_status_name': 'object',\n",
    "    'edit_status': 'object',\n",
    "    'sequence_number': 'object',  # Using object due to potential NaNs\n",
    "    'population': 'float64',\n",
    "    'minority_population': 'float64',\n",
    "    'hud_median_family_income': 'float64',\n",
    "    'tract_to_msamd_income': 'float64',\n",
    "    'number_of_owner_occupied_units': 'float64',\n",
    "    'number_of_1_to_4_family_units': 'float64',\n",
    "    'application_date_indicator': 'object'\n",
    "}\n",
    "\n",
    "# Read the CSV file using Dask with specified data types\n",
    "ddf = dd.read_csv(file_path, dtype=dtype)\n",
    "\n",
    "# Replace empty strings with None (Dask will handle this lazily)\n",
    "ddf = ddf.replace({'': None})\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "ddf.to_csv(cleaned_file_path, index=False, single_file=True)\n",
    "\n",
    "print(f\"Cleaned file saved to {cleaned_file_path}\")\n",
    "\n",
    "print(ddf.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
